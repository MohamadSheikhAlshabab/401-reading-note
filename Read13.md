# Read: Class 13 [URL](https://github.com/MohamadSheikhAlshabab/401-reading-note/blob/master/Read01.md)

## How to run Linear regression in Python scikit-Learn:
 - Scikit-learn is a powerful Python module for machine learning.
 - It contains function for regression, classification, clustering, model selection and dimensionality reduction.
 - Important functions of a linear regression model are:
  - 1 - lm.fit() -> fits a linear model
  - 2 - lm.predict() -> Predict Y using the linear model with estimated coefficients
  - 3 - lm.score() -> Returns the coefficient of determination (R^2). A measure of how well observed outcomes are replicated by the model, as the proportion of total variation of outcomes explained by the model.
  
### Residual Plots: 
 - Residual plots are a good way to visualize the errors in your data. If you have done a good job then your data should be randomly scattered around line zero. 
 - ![img](https://bigdata-madesimple.com/wp-content/uploads/2016/04/Plt-scatter.png)
 - ![img](https://bigdata-madesimple.com/wp-content/uploads/2016/04/Residual-plot.png)
 
### Conclusion:
- 1 - I explored the boston data set and then renamed its column names.
- 2 - I explored the boston data set using .DESCR, my goal was to predict the housing prices using the given features.
- 3 - I used Scikit learn to fit linear regression to the entire data set and calculated the mean squared error.
- 4 - I made a train-test split and calculated the mean squared error for my training data and test data.
- 5 - I then plotted the residuals for my training and test datasets.

----
## Linear Regression in Python:
 - Linear regression is one of the fundamental statistical and machine learning techniques.
 
 ### Regression:
  - Regression analysis is one of the most important fields in statistics and machine learning.
  - There are many regression methods available. Linear regression is one of them.
  - in regression analysis, you usually consider some phenomenon of interest and have a number of observations. 
  - Each observation has two or more features.
  - Following the assumption that (at least) one of the features depends on the others, you try to establish a relation among them.
  - __you need to find a function that maps some features or variables to others sufficiently well.__
  - The dependent features are called __the dependent variables, outputs, or responses__.
  - The independent features are called __the independent variables, inputs, or predictors__.
  
### Linear Regression:
 - __Problem Formulation__: 
  - some dependent variable ğ‘¦ on the set of independent variables ğ± = (ğ‘¥â‚, â€¦, ğ‘¥áµ£), where ğ‘Ÿ is the number of predictors, you assume a linear relationship between ğ‘¦ and ğ±: ğ‘¦ = ğ›½â‚€ + ğ›½â‚ğ‘¥â‚ + â‹¯ + ğ›½áµ£ğ‘¥áµ£ + ğœ€.
  - This equation is the regression equation. ğ›½â‚€, ğ›½â‚, â€¦, ğ›½áµ£ are the regression coefficients, and ğœ€ is the random error.
  - calculates the estimators of the regression coefficients or simply the predicted weights, denoted with ğ‘â‚€, ğ‘â‚, â€¦, ğ‘áµ£. They define the estimated regression function ğ‘“(ğ±) = ğ‘â‚€ + ğ‘â‚ğ‘¥â‚ + â‹¯ + ğ‘áµ£ğ‘¥áµ£.
  - The estimated or predicted response, ğ‘“(ğ±áµ¢), for each observation ğ‘– = 1, â€¦, ğ‘›, should be as close as possible to the corresponding actual response ğ‘¦áµ¢. 
  - The differences ğ‘¦áµ¢ - ğ‘“(ğ±áµ¢) for all observations ğ‘– = 1, â€¦, ğ‘›, are called the residuals.
  - Regression is about determining the best predicted weights, that is the weights corresponding to the smallest residuals.
  - To get the best weights, you usually minimize the sum of squared residuals (SSR) for all observations ğ‘– = 1, â€¦, ğ‘›: SSR = Î£áµ¢(ğ‘¦áµ¢ - ğ‘“(ğ±áµ¢))Â². This approach is called the method of ordinary least squares.
- __Simple Linear Regression__:
 -  is the simplest case of linear regression with a single independent variable, ğ± = ğ‘¥.
 ![img](https://files.realpython.com/media/fig-lin-reg.a506035b654a.png)
- __Multiple Linear Regression__:
 -  is a case of linear regression with two or more independent variables.
 - The estimated regression function is ğ‘“(ğ‘¥â‚, â€¦, ğ‘¥áµ£) = ğ‘â‚€ + ğ‘â‚ğ‘¥â‚ + â‹¯ +ğ‘áµ£ğ‘¥áµ£, and there are ğ‘Ÿ + 1 weights to be determined when the number of inputs is ğ‘Ÿ.
- __Polynomial Regression__:
 - a generalized case of linear regression. 
 - in addition to linear terms like ğ‘â‚ğ‘¥â‚, your regression function ğ‘“ can include non-linear terms such as ğ‘â‚‚ğ‘¥â‚Â², ğ‘â‚ƒğ‘¥â‚Â³, or even ğ‘â‚„ğ‘¥â‚ğ‘¥â‚‚, ğ‘â‚…ğ‘¥â‚Â²ğ‘¥â‚‚, and so on.
 - The simplest example of polynomial regression has a single independent variable, and the estimated regression function is a polynomial of degree 2: ğ‘“(ğ‘¥) = ğ‘â‚€ + ğ‘â‚ğ‘¥ + ğ‘â‚‚ğ‘¥Â².
 - you can solve __the polynomial regression problem as a linear problem with__ the term ğ‘¥Â² regarded as an input variable.
- __Underfitting and Overfitting__:
 -  the choice of the optimal degree of the polynomial regression function:  __underfitting and overfitting__.
 - __Underfitting__ occurs when a model canâ€™t accurately capture the dependencies among data, usually as a consequence of its own simplicity.
  - It often yields a low ğ‘…Â² with known data and bad generalization capabilities when applied with new data.
 - __Overfitting__ happens when a model learns both dependencies among data and random fluctuations.
  - When applied to known data, such models usually yield high ğ‘…Â². However, they often donâ€™t generalize well and have significantly lower ğ‘…Â² when used with new data.
  ![img](https://files.realpython.com/media/poly-reg.5790f47603d8.png) 
---
 ### Types of Linear Regression:
 - 1 - Simple linear regression: 1 dependent variable (interval or ratio), 1 independent variable (interval or ratio or dichotomous)
 - 2 - Multiple linear regression: 1 dependent variable (interval or ratio) , 2+ independent variables (interval or ratio or dichotomous)
 - 3 - Logistic regression: 1 dependent variable (dichotomous), 2+ independent variable(s) (interval or ratio or dichotomous)
 - 4 - Ordinal regression: 1 dependent variable (ordinal), 1+ independent variable(s) (nominal or dichotomous)
 - 5 - Multinomial regression: 1 dependent variable (nominal), 1+ independent variable(s) (interval or ratio or dichotomous)
 - 6 - Discriminant analysis:1 dependent variable (nominal), 1+ independent variable(s) (interval or ratio)

   
